{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prospecção de Dados 2022/2023\n",
    "## First Home Assignment\n",
    "### David Conceição 52518, Tomás Matos 53438, Rudolfo Faria 44252"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "##### Divide the train.csv data into train and test set. Do svd in the training set. train the model with the svd matrix result and evaluate the model with the test set\n",
    "##### With pca we transform both the training and the testing set to use only the principal component(we do not need to know wich are the component)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the UCI Supercoductivity Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21263, 82) (21263, 88) (21263, 167)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>Ir</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Hg</th>\n",
       "      <th>Tl</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Bi</th>\n",
       "      <th>Po</th>\n",
       "      <th>At</th>\n",
       "      <th>Rn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.862692</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.116612</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.062396</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>31.794921</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>92.729214</td>\n",
       "      <td>58.518416</td>\n",
       "      <td>73.132787</td>\n",
       "      <td>36.396602</td>\n",
       "      <td>1.449309</td>\n",
       "      <td>1.057755</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>36.161939</td>\n",
       "      <td>47.094633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.885242</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.122509</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>35.741099</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.873967</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.119560</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.022291</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>33.768010</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.840143</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.110716</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.129224</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>27.848743</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21258</th>\n",
       "      <td>4</td>\n",
       "      <td>106.957877</td>\n",
       "      <td>53.095769</td>\n",
       "      <td>82.515384</td>\n",
       "      <td>43.135565</td>\n",
       "      <td>1.177145</td>\n",
       "      <td>1.254119</td>\n",
       "      <td>146.88130</td>\n",
       "      <td>15.504479</td>\n",
       "      <td>65.764081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21259</th>\n",
       "      <td>5</td>\n",
       "      <td>92.266740</td>\n",
       "      <td>49.021367</td>\n",
       "      <td>64.812662</td>\n",
       "      <td>32.867748</td>\n",
       "      <td>1.323287</td>\n",
       "      <td>1.571630</td>\n",
       "      <td>188.38390</td>\n",
       "      <td>7.353333</td>\n",
       "      <td>69.232655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21260</th>\n",
       "      <td>2</td>\n",
       "      <td>99.663190</td>\n",
       "      <td>95.609104</td>\n",
       "      <td>99.433882</td>\n",
       "      <td>95.464320</td>\n",
       "      <td>0.690847</td>\n",
       "      <td>0.530198</td>\n",
       "      <td>13.51362</td>\n",
       "      <td>53.041104</td>\n",
       "      <td>6.756810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21261</th>\n",
       "      <td>2</td>\n",
       "      <td>99.663190</td>\n",
       "      <td>97.095602</td>\n",
       "      <td>99.433882</td>\n",
       "      <td>96.901083</td>\n",
       "      <td>0.690847</td>\n",
       "      <td>0.640883</td>\n",
       "      <td>13.51362</td>\n",
       "      <td>31.115202</td>\n",
       "      <td>6.756810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21262</th>\n",
       "      <td>3</td>\n",
       "      <td>87.468333</td>\n",
       "      <td>86.858500</td>\n",
       "      <td>82.555758</td>\n",
       "      <td>80.458722</td>\n",
       "      <td>1.041270</td>\n",
       "      <td>0.895229</td>\n",
       "      <td>71.75500</td>\n",
       "      <td>43.144000</td>\n",
       "      <td>29.905282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21263 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
       "0                       4         88.944468             57.862692   \n",
       "1                       5         92.729214             58.518416   \n",
       "2                       4         88.944468             57.885242   \n",
       "3                       4         88.944468             57.873967   \n",
       "4                       4         88.944468             57.840143   \n",
       "...                   ...               ...                   ...   \n",
       "21258                   4        106.957877             53.095769   \n",
       "21259                   5         92.266740             49.021367   \n",
       "21260                   2         99.663190             95.609104   \n",
       "21261                   2         99.663190             97.095602   \n",
       "21262                   3         87.468333             86.858500   \n",
       "\n",
       "       gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
       "0              66.361592              36.116612             1.181795   \n",
       "1              73.132787              36.396602             1.449309   \n",
       "2              66.361592              36.122509             1.181795   \n",
       "3              66.361592              36.119560             1.181795   \n",
       "4              66.361592              36.110716             1.181795   \n",
       "...                  ...                    ...                  ...   \n",
       "21258          82.515384              43.135565             1.177145   \n",
       "21259          64.812662              32.867748             1.323287   \n",
       "21260          99.433882              95.464320             0.690847   \n",
       "21261          99.433882              96.901083             0.690847   \n",
       "21262          82.555758              80.458722             1.041270   \n",
       "\n",
       "       wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
       "0                     1.062396          122.90607              31.794921   \n",
       "1                     1.057755          122.90607              36.161939   \n",
       "2                     0.975980          122.90607              35.741099   \n",
       "3                     1.022291          122.90607              33.768010   \n",
       "4                     1.129224          122.90607              27.848743   \n",
       "...                        ...                ...                    ...   \n",
       "21258                 1.254119          146.88130              15.504479   \n",
       "21259                 1.571630          188.38390               7.353333   \n",
       "21260                 0.530198           13.51362              53.041104   \n",
       "21261                 0.640883           13.51362              31.115202   \n",
       "21262                 0.895229           71.75500              43.144000   \n",
       "\n",
       "       std_atomic_mass  ...   Ir   Pt   Au   Hg   Tl   Pb   Bi  Po  At  Rn  \n",
       "0            51.968828  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0  \n",
       "1            47.094633  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0  \n",
       "2            51.968828  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0  \n",
       "3            51.968828  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0  \n",
       "4            51.968828  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0  \n",
       "...                ...  ...  ...  ...  ...  ...  ...  ...  ...  ..  ..  ..  \n",
       "21258        65.764081  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0  \n",
       "21259        69.232655  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0   0   0   0  \n",
       "21260         6.756810  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0  \n",
       "21261         6.756810  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0  \n",
       "21262        29.905282  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0  \n",
       "\n",
       "[21263 rows x 167 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the train and unique csv files \n",
    "train_data = pd.read_csv('train.csv')\n",
    "unique_data = pd.read_csv('unique_m.csv')\n",
    "\n",
    "# merge the files together\n",
    "merged_dataset = pd.concat([train_data,unique_data],axis=1)\n",
    "\n",
    "# remove critical_temp and material columns\n",
    "dataset = merged_dataset.drop(['critical_temp','material'],axis=1)\n",
    "print(train_data.shape,unique_data.shape,dataset.shape)\n",
    "pd.DataFrame(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29.   26.   19.   ...  1.98  1.84 12.8 ]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(dataset.values)\n",
    "y = np.array(train_data.values[:,-1])\n",
    "(X.shape,y.shape)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dividing the dataset into training and testing set. Scaling the X_train and X_test\n",
    "##### We want to aply SVD to the training set so we have to transform the matrix with the independent variables and the depend one also???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14246, 167)\n",
      "y_train shape: (14246,)\n",
      "X_test shape: (7017, 167)\n",
      "y_test shape: (7017,)\n",
      "train_set shape: (14246, 168)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print('X_train shape: ' + str(X_train.shape))\n",
    "print('y_train shape: ' + str((y_train.shape)))\n",
    "print('X_test shape: ' + str((X_test.shape)))\n",
    "print('y_test shape: ' + str((y_test.shape)))\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# merge the X_train and y_train so that we can put in the svd\n",
    "train_set = np.append(np.array(X_train), np.array(y_train).reshape(-1,1), axis=1)\n",
    "print('train_set shape: ' + str((train_set.shape)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 1 - Dimensionality reduction\n",
    "### Perform dimensionality reduction (preferably PCA or SVD) and analize the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "##### Problem: i dont understand why im getting a same size matrix with the SVD when it should perform Dimensionality Reduction\n",
    "##### Define a set of functions to transform a given array into a new one using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the U,S and V matrices. \n",
    "# Also gives the combined importance of the components\n",
    "def toSVD(data):\n",
    "    u,s,v = np.linalg.svd(data)\n",
    "    for i in range(len(s)):\n",
    "        print(\"first %d components have a combined importance of %7.4f\" %(i+1, s[:i+1].sum()/s.sum()))\n",
    "    return [u,s,v]\n",
    "\n",
    "# Function that calculates the new matrix (U*S*V) with a given number of components\n",
    "def components(u,s,v, n_components):\n",
    "    U = u[:,:n_components]\n",
    "    S = np.diag(s[:n_components]) #presented in matrix diagonal format\n",
    "    V = v[:n_components, :]\n",
    "    return U @ S @ V\n",
    "\n",
    "# Function that calculates the MSE and rmse given the original and SVD matrices\n",
    "def evaluateSVD(old_matrix, new_matrix):\n",
    "    MSE=((old_matrix - new_matrix)**2).mean()\n",
    "    rmse=(np.sqrt((old_matrix - new_matrix)**2)).mean()\n",
    "    return [MSE,rmse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 1 components have a combined importance of  0.5411\n",
      "first 2 components have a combined importance of  0.7634\n",
      "first 3 components have a combined importance of  0.8328\n",
      "first 4 components have a combined importance of  0.8707\n",
      "first 5 components have a combined importance of  0.9022\n",
      "first 6 components have a combined importance of  0.9253\n",
      "first 7 components have a combined importance of  0.9432\n",
      "first 8 components have a combined importance of  0.9551\n",
      "first 9 components have a combined importance of  0.9634\n",
      "first 10 components have a combined importance of  0.9683\n",
      "first 11 components have a combined importance of  0.9726\n",
      "first 12 components have a combined importance of  0.9764\n",
      "first 13 components have a combined importance of  0.9785\n",
      "first 14 components have a combined importance of  0.9804\n",
      "first 15 components have a combined importance of  0.9823\n",
      "first 16 components have a combined importance of  0.9839\n",
      "first 17 components have a combined importance of  0.9853\n",
      "first 18 components have a combined importance of  0.9865\n",
      "first 19 components have a combined importance of  0.9877\n",
      "first 20 components have a combined importance of  0.9887\n",
      "first 21 components have a combined importance of  0.9896\n",
      "first 22 components have a combined importance of  0.9904\n",
      "first 23 components have a combined importance of  0.9912\n",
      "first 24 components have a combined importance of  0.9918\n",
      "first 25 components have a combined importance of  0.9923\n",
      "first 26 components have a combined importance of  0.9929\n",
      "first 27 components have a combined importance of  0.9934\n",
      "first 28 components have a combined importance of  0.9938\n",
      "first 29 components have a combined importance of  0.9942\n",
      "first 30 components have a combined importance of  0.9946\n",
      "first 31 components have a combined importance of  0.9949\n",
      "first 32 components have a combined importance of  0.9952\n",
      "first 33 components have a combined importance of  0.9954\n",
      "first 34 components have a combined importance of  0.9957\n",
      "first 35 components have a combined importance of  0.9959\n",
      "first 36 components have a combined importance of  0.9961\n",
      "first 37 components have a combined importance of  0.9963\n",
      "first 38 components have a combined importance of  0.9964\n",
      "first 39 components have a combined importance of  0.9966\n",
      "first 40 components have a combined importance of  0.9968\n",
      "first 41 components have a combined importance of  0.9969\n",
      "first 42 components have a combined importance of  0.9970\n",
      "first 43 components have a combined importance of  0.9972\n",
      "first 44 components have a combined importance of  0.9973\n",
      "first 45 components have a combined importance of  0.9974\n",
      "first 46 components have a combined importance of  0.9975\n",
      "first 47 components have a combined importance of  0.9976\n",
      "first 48 components have a combined importance of  0.9977\n",
      "first 49 components have a combined importance of  0.9978\n",
      "first 50 components have a combined importance of  0.9979\n",
      "first 51 components have a combined importance of  0.9980\n",
      "first 52 components have a combined importance of  0.9981\n",
      "first 53 components have a combined importance of  0.9982\n",
      "first 54 components have a combined importance of  0.9983\n",
      "first 55 components have a combined importance of  0.9983\n",
      "first 56 components have a combined importance of  0.9984\n",
      "first 57 components have a combined importance of  0.9985\n",
      "first 58 components have a combined importance of  0.9985\n",
      "first 59 components have a combined importance of  0.9986\n",
      "first 60 components have a combined importance of  0.9986\n",
      "first 61 components have a combined importance of  0.9987\n",
      "first 62 components have a combined importance of  0.9987\n",
      "first 63 components have a combined importance of  0.9988\n",
      "first 64 components have a combined importance of  0.9988\n",
      "first 65 components have a combined importance of  0.9989\n",
      "first 66 components have a combined importance of  0.9989\n",
      "first 67 components have a combined importance of  0.9990\n",
      "first 68 components have a combined importance of  0.9990\n",
      "first 69 components have a combined importance of  0.9990\n",
      "first 70 components have a combined importance of  0.9991\n",
      "first 71 components have a combined importance of  0.9991\n",
      "first 72 components have a combined importance of  0.9991\n",
      "first 73 components have a combined importance of  0.9992\n",
      "first 74 components have a combined importance of  0.9992\n",
      "first 75 components have a combined importance of  0.9992\n",
      "first 76 components have a combined importance of  0.9993\n",
      "first 77 components have a combined importance of  0.9993\n",
      "first 78 components have a combined importance of  0.9993\n",
      "first 79 components have a combined importance of  0.9994\n",
      "first 80 components have a combined importance of  0.9994\n",
      "first 81 components have a combined importance of  0.9994\n",
      "first 82 components have a combined importance of  0.9994\n",
      "first 83 components have a combined importance of  0.9995\n",
      "first 84 components have a combined importance of  0.9995\n",
      "first 85 components have a combined importance of  0.9995\n",
      "first 86 components have a combined importance of  0.9995\n",
      "first 87 components have a combined importance of  0.9996\n",
      "first 88 components have a combined importance of  0.9996\n",
      "first 89 components have a combined importance of  0.9996\n",
      "first 90 components have a combined importance of  0.9996\n",
      "first 91 components have a combined importance of  0.9996\n",
      "first 92 components have a combined importance of  0.9996\n",
      "first 93 components have a combined importance of  0.9997\n",
      "first 94 components have a combined importance of  0.9997\n",
      "first 95 components have a combined importance of  0.9997\n",
      "first 96 components have a combined importance of  0.9997\n",
      "first 97 components have a combined importance of  0.9997\n",
      "first 98 components have a combined importance of  0.9997\n",
      "first 99 components have a combined importance of  0.9997\n",
      "first 100 components have a combined importance of  0.9997\n",
      "first 101 components have a combined importance of  0.9998\n",
      "first 102 components have a combined importance of  0.9998\n",
      "first 103 components have a combined importance of  0.9998\n",
      "first 104 components have a combined importance of  0.9998\n",
      "first 105 components have a combined importance of  0.9998\n",
      "first 106 components have a combined importance of  0.9998\n",
      "first 107 components have a combined importance of  0.9998\n",
      "first 108 components have a combined importance of  0.9998\n",
      "first 109 components have a combined importance of  0.9998\n",
      "first 110 components have a combined importance of  0.9998\n",
      "first 111 components have a combined importance of  0.9998\n",
      "first 112 components have a combined importance of  0.9998\n",
      "first 113 components have a combined importance of  0.9999\n",
      "first 114 components have a combined importance of  0.9999\n",
      "first 115 components have a combined importance of  0.9999\n",
      "first 116 components have a combined importance of  0.9999\n",
      "first 117 components have a combined importance of  0.9999\n",
      "first 118 components have a combined importance of  0.9999\n",
      "first 119 components have a combined importance of  0.9999\n",
      "first 120 components have a combined importance of  0.9999\n",
      "first 121 components have a combined importance of  0.9999\n",
      "first 122 components have a combined importance of  0.9999\n",
      "first 123 components have a combined importance of  0.9999\n",
      "first 124 components have a combined importance of  0.9999\n",
      "first 125 components have a combined importance of  0.9999\n",
      "first 126 components have a combined importance of  0.9999\n",
      "first 127 components have a combined importance of  0.9999\n",
      "first 128 components have a combined importance of  0.9999\n",
      "first 129 components have a combined importance of  0.9999\n",
      "first 130 components have a combined importance of  0.9999\n",
      "first 131 components have a combined importance of  0.9999\n",
      "first 132 components have a combined importance of  0.9999\n",
      "first 133 components have a combined importance of  1.0000\n",
      "first 134 components have a combined importance of  1.0000\n",
      "first 135 components have a combined importance of  1.0000\n",
      "first 136 components have a combined importance of  1.0000\n",
      "first 137 components have a combined importance of  1.0000\n",
      "first 138 components have a combined importance of  1.0000\n",
      "first 139 components have a combined importance of  1.0000\n",
      "first 140 components have a combined importance of  1.0000\n",
      "first 141 components have a combined importance of  1.0000\n",
      "first 142 components have a combined importance of  1.0000\n",
      "first 143 components have a combined importance of  1.0000\n",
      "first 144 components have a combined importance of  1.0000\n",
      "first 145 components have a combined importance of  1.0000\n",
      "first 146 components have a combined importance of  1.0000\n",
      "first 147 components have a combined importance of  1.0000\n",
      "first 148 components have a combined importance of  1.0000\n",
      "first 149 components have a combined importance of  1.0000\n",
      "first 150 components have a combined importance of  1.0000\n",
      "first 151 components have a combined importance of  1.0000\n",
      "first 152 components have a combined importance of  1.0000\n",
      "first 153 components have a combined importance of  1.0000\n",
      "first 154 components have a combined importance of  1.0000\n",
      "first 155 components have a combined importance of  1.0000\n",
      "first 156 components have a combined importance of  1.0000\n",
      "first 157 components have a combined importance of  1.0000\n",
      "first 158 components have a combined importance of  1.0000\n",
      "first 159 components have a combined importance of  1.0000\n",
      "first 160 components have a combined importance of  1.0000\n",
      "first 161 components have a combined importance of  1.0000\n",
      "first 162 components have a combined importance of  1.0000\n",
      "first 163 components have a combined importance of  1.0000\n",
      "first 164 components have a combined importance of  1.0000\n",
      "first 165 components have a combined importance of  1.0000\n",
      "first 166 components have a combined importance of  1.0000\n",
      "first 167 components have a combined importance of  1.0000\n",
      "first 168 components have a combined importance of  1.0000\n"
     ]
    }
   ],
   "source": [
    "# Transform the train_array\n",
    "new = toSVD(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14246, 168)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = components(new[0], new[1], new[2],5)\n",
    "d.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bellow we can see that by increasing the number of components the MSE and rmse will decrease. The goal of SVD is to reduce the dimensionality of a dataset while preserving as much information as possible therefore, we will have to find the best combination between number of components and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1 components:\n",
      "The MSE is: 266979.8057\n",
      "The rsme is: 103.3352\n",
      " \n",
      "With 5 components:\n",
      "The MSE is: 5286.1250\n",
      "The rsme is:  18.3016\n",
      " \n",
      "With 10 components:\n",
      "The MSE is: 265.4524\n",
      "The rsme is:   5.7877\n",
      " \n",
      "With 20 components:\n",
      "The MSE is: 20.3811\n",
      "The rsme is:   1.7010\n",
      " \n",
      "With 30 components:\n",
      "The MSE is:  3.4222\n",
      "The rsme is:   0.7232\n",
      " \n",
      "With 50 components:\n",
      "The MSE is:  0.4066\n",
      "The rsme is:   0.2101\n",
      " \n",
      "With 70 components:\n",
      "The MSE is:  0.0863\n",
      "The rsme is:   0.0960\n",
      " \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_components = [1,5,10,20,30,50,70]\n",
    "RMSEs=[]\n",
    "for n in n_components:\n",
    "    print(\"With %d components:\"%(n))\n",
    "    MSE = evaluateSVD(train_set,components(new[0], new[1], new[2],n))[0]\n",
    "    rmse = evaluateSVD(train_set,components(new[0], new[1], new[2],n))[1]\n",
    "    RMSEs.append(rmse)\n",
    "    print(\"The MSE is: %7.4f\\nThe rsme is: %8.4f\" % (MSE, rmse))\n",
    "    print(\" \")\n",
    "\n",
    "plt.plot(n_components, RMSEs, label=\"SVD\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC0 - Variance explained:  0.2086 - Total Variance:  0.2086\n",
      "PC1 - Variance explained:  0.0581 - Total Variance:  0.2667\n",
      "PC2 - Variance explained:  0.0536 - Total Variance:  0.3203\n",
      "PC3 - Variance explained:  0.0432 - Total Variance:  0.3635\n",
      "PC4 - Variance explained:  0.0335 - Total Variance:  0.3970\n",
      "PC5 - Variance explained:  0.0225 - Total Variance:  0.4194\n",
      "PC6 - Variance explained:  0.0213 - Total Variance:  0.4407\n",
      "PC7 - Variance explained:  0.0183 - Total Variance:  0.4590\n",
      "PC8 - Variance explained:  0.0160 - Total Variance:  0.4750\n",
      "PC9 - Variance explained:  0.0150 - Total Variance:  0.4900\n",
      "PC10 - Variance explained:  0.0137 - Total Variance:  0.5036\n",
      "PC11 - Variance explained:  0.0128 - Total Variance:  0.5164\n",
      "PC12 - Variance explained:  0.0121 - Total Variance:  0.5286\n",
      "PC13 - Variance explained:  0.0116 - Total Variance:  0.5402\n",
      "PC14 - Variance explained:  0.0113 - Total Variance:  0.5514\n",
      "PC15 - Variance explained:  0.0109 - Total Variance:  0.5623\n",
      "PC16 - Variance explained:  0.0107 - Total Variance:  0.5730\n",
      "PC17 - Variance explained:  0.0102 - Total Variance:  0.5832\n",
      "PC18 - Variance explained:  0.0099 - Total Variance:  0.5931\n",
      "PC19 - Variance explained:  0.0092 - Total Variance:  0.6024\n",
      "PC20 - Variance explained:  0.0090 - Total Variance:  0.6114\n",
      "PC21 - Variance explained:  0.0087 - Total Variance:  0.6201\n",
      "PC22 - Variance explained:  0.0085 - Total Variance:  0.6286\n",
      "PC23 - Variance explained:  0.0084 - Total Variance:  0.6370\n",
      "PC24 - Variance explained:  0.0081 - Total Variance:  0.6451\n",
      "PC25 - Variance explained:  0.0081 - Total Variance:  0.6532\n",
      "PC26 - Variance explained:  0.0077 - Total Variance:  0.6609\n",
      "PC27 - Variance explained:  0.0076 - Total Variance:  0.6686\n",
      "PC28 - Variance explained:  0.0074 - Total Variance:  0.6760\n",
      "PC29 - Variance explained:  0.0074 - Total Variance:  0.6833\n",
      "PC30 - Variance explained:  0.0073 - Total Variance:  0.6906\n",
      "PC31 - Variance explained:  0.0071 - Total Variance:  0.6977\n",
      "PC32 - Variance explained:  0.0070 - Total Variance:  0.7047\n",
      "PC33 - Variance explained:  0.0069 - Total Variance:  0.7116\n",
      "PC34 - Variance explained:  0.0067 - Total Variance:  0.7184\n",
      "PC35 - Variance explained:  0.0066 - Total Variance:  0.7250\n",
      "PC36 - Variance explained:  0.0066 - Total Variance:  0.7316\n",
      "PC37 - Variance explained:  0.0066 - Total Variance:  0.7382\n",
      "PC38 - Variance explained:  0.0065 - Total Variance:  0.7448\n",
      "PC39 - Variance explained:  0.0065 - Total Variance:  0.7512\n",
      "PC40 - Variance explained:  0.0064 - Total Variance:  0.7577\n",
      "PC41 - Variance explained:  0.0064 - Total Variance:  0.7641\n",
      "PC42 - Variance explained:  0.0064 - Total Variance:  0.7705\n",
      "PC43 - Variance explained:  0.0064 - Total Variance:  0.7768\n",
      "PC44 - Variance explained:  0.0064 - Total Variance:  0.7832\n",
      "PC45 - Variance explained:  0.0063 - Total Variance:  0.7895\n",
      "PC46 - Variance explained:  0.0063 - Total Variance:  0.7958\n",
      "PC47 - Variance explained:  0.0063 - Total Variance:  0.8021\n",
      "PC48 - Variance explained:  0.0063 - Total Variance:  0.8083\n",
      "PC49 - Variance explained:  0.0062 - Total Variance:  0.8145\n",
      "PC50 - Variance explained:  0.0061 - Total Variance:  0.8207\n",
      "PC51 - Variance explained:  0.0061 - Total Variance:  0.8268\n",
      "PC52 - Variance explained:  0.0061 - Total Variance:  0.8328\n",
      "PC53 - Variance explained:  0.0060 - Total Variance:  0.8388\n",
      "PC54 - Variance explained:  0.0060 - Total Variance:  0.8448\n",
      "PC55 - Variance explained:  0.0059 - Total Variance:  0.8506\n",
      "PC56 - Variance explained:  0.0058 - Total Variance:  0.8565\n",
      "PC57 - Variance explained:  0.0057 - Total Variance:  0.8622\n",
      "PC58 - Variance explained:  0.0057 - Total Variance:  0.8678\n",
      "PC59 - Variance explained:  0.0056 - Total Variance:  0.8734\n",
      "PC60 - Variance explained:  0.0054 - Total Variance:  0.8788\n",
      "PC61 - Variance explained:  0.0053 - Total Variance:  0.8841\n",
      "PC62 - Variance explained:  0.0052 - Total Variance:  0.8893\n",
      "PC63 - Variance explained:  0.0050 - Total Variance:  0.8944\n",
      "PC64 - Variance explained:  0.0050 - Total Variance:  0.8993\n",
      "PC65 - Variance explained:  0.0049 - Total Variance:  0.9042\n",
      "PC66 - Variance explained:  0.0048 - Total Variance:  0.9091\n",
      "PC67 - Variance explained:  0.0047 - Total Variance:  0.9138\n",
      "PC68 - Variance explained:  0.0046 - Total Variance:  0.9184\n",
      "PC69 - Variance explained:  0.0046 - Total Variance:  0.9230\n",
      "PC70 - Variance explained:  0.0045 - Total Variance:  0.9275\n",
      "PC71 - Variance explained:  0.0041 - Total Variance:  0.9316\n",
      "PC72 - Variance explained:  0.0039 - Total Variance:  0.9355\n",
      "PC73 - Variance explained:  0.0038 - Total Variance:  0.9393\n",
      "PC74 - Variance explained:  0.0036 - Total Variance:  0.9430\n",
      "PC75 - Variance explained:  0.0035 - Total Variance:  0.9465\n",
      "PC76 - Variance explained:  0.0032 - Total Variance:  0.9497\n",
      "PC77 - Variance explained:  0.0032 - Total Variance:  0.9529\n",
      "PC78 - Variance explained:  0.0031 - Total Variance:  0.9560\n",
      "PC79 - Variance explained:  0.0029 - Total Variance:  0.9589\n",
      "\n",
      "PC0 - Variance explained:  0.6497 - Total Variance:  0.6497\n",
      "PC1 - Variance explained:  0.0770 - Total Variance:  0.7267\n",
      "PC2 - Variance explained:  0.0213 - Total Variance:  0.7481\n",
      "PC3 - Variance explained:  0.0199 - Total Variance:  0.7679\n",
      "PC4 - Variance explained:  0.0161 - Total Variance:  0.7841\n",
      "PC5 - Variance explained:  0.0127 - Total Variance:  0.7967\n",
      "PC6 - Variance explained:  0.0097 - Total Variance:  0.8065\n",
      "PC7 - Variance explained:  0.0085 - Total Variance:  0.8150\n",
      "PC8 - Variance explained:  0.0079 - Total Variance:  0.8229\n",
      "PC9 - Variance explained:  0.0078 - Total Variance:  0.8307\n",
      "PC10 - Variance explained:  0.0071 - Total Variance:  0.8378\n",
      "PC11 - Variance explained:  0.0062 - Total Variance:  0.8440\n",
      "PC12 - Variance explained:  0.0058 - Total Variance:  0.8499\n",
      "PC13 - Variance explained:  0.0054 - Total Variance:  0.8552\n",
      "PC14 - Variance explained:  0.0049 - Total Variance:  0.8601\n",
      "PC15 - Variance explained:  0.0048 - Total Variance:  0.8649\n",
      "PC16 - Variance explained:  0.0046 - Total Variance:  0.8695\n",
      "PC17 - Variance explained:  0.0044 - Total Variance:  0.8738\n",
      "PC18 - Variance explained:  0.0040 - Total Variance:  0.8779\n",
      "PC19 - Variance explained:  0.0037 - Total Variance:  0.8816\n",
      "PC20 - Variance explained:  0.0037 - Total Variance:  0.8853\n",
      "PC21 - Variance explained:  0.0035 - Total Variance:  0.8888\n",
      "PC22 - Variance explained:  0.0035 - Total Variance:  0.8922\n",
      "PC23 - Variance explained:  0.0033 - Total Variance:  0.8956\n",
      "PC24 - Variance explained:  0.0032 - Total Variance:  0.8988\n",
      "PC25 - Variance explained:  0.0031 - Total Variance:  0.9018\n",
      "PC26 - Variance explained:  0.0029 - Total Variance:  0.9047\n",
      "PC27 - Variance explained:  0.0029 - Total Variance:  0.9076\n",
      "PC28 - Variance explained:  0.0028 - Total Variance:  0.9103\n",
      "PC29 - Variance explained:  0.0027 - Total Variance:  0.9131\n",
      "PC30 - Variance explained:  0.0027 - Total Variance:  0.9158\n",
      "PC31 - Variance explained:  0.0027 - Total Variance:  0.9184\n",
      "PC32 - Variance explained:  0.0025 - Total Variance:  0.9209\n",
      "PC33 - Variance explained:  0.0025 - Total Variance:  0.9234\n",
      "PC34 - Variance explained:  0.0024 - Total Variance:  0.9258\n",
      "PC35 - Variance explained:  0.0024 - Total Variance:  0.9282\n",
      "PC36 - Variance explained:  0.0023 - Total Variance:  0.9305\n",
      "PC37 - Variance explained:  0.0023 - Total Variance:  0.9328\n",
      "PC38 - Variance explained:  0.0022 - Total Variance:  0.9349\n",
      "PC39 - Variance explained:  0.0022 - Total Variance:  0.9371\n",
      "PC40 - Variance explained:  0.0021 - Total Variance:  0.9392\n",
      "PC41 - Variance explained:  0.0021 - Total Variance:  0.9413\n",
      "PC42 - Variance explained:  0.0020 - Total Variance:  0.9433\n",
      "PC43 - Variance explained:  0.0020 - Total Variance:  0.9453\n",
      "PC44 - Variance explained:  0.0019 - Total Variance:  0.9472\n",
      "PC45 - Variance explained:  0.0019 - Total Variance:  0.9491\n",
      "PC46 - Variance explained:  0.0019 - Total Variance:  0.9510\n",
      "PC47 - Variance explained:  0.0018 - Total Variance:  0.9528\n",
      "PC48 - Variance explained:  0.0018 - Total Variance:  0.9546\n",
      "PC49 - Variance explained:  0.0017 - Total Variance:  0.9563\n",
      "PC50 - Variance explained:  0.0017 - Total Variance:  0.9579\n",
      "PC51 - Variance explained:  0.0016 - Total Variance:  0.9596\n",
      "PC52 - Variance explained:  0.0016 - Total Variance:  0.9612\n",
      "PC53 - Variance explained:  0.0016 - Total Variance:  0.9628\n",
      "PC54 - Variance explained:  0.0015 - Total Variance:  0.9643\n",
      "PC55 - Variance explained:  0.0015 - Total Variance:  0.9658\n",
      "PC56 - Variance explained:  0.0014 - Total Variance:  0.9672\n",
      "PC57 - Variance explained:  0.0014 - Total Variance:  0.9686\n",
      "PC58 - Variance explained:  0.0014 - Total Variance:  0.9700\n",
      "PC59 - Variance explained:  0.0013 - Total Variance:  0.9713\n",
      "PC60 - Variance explained:  0.0013 - Total Variance:  0.9726\n",
      "PC61 - Variance explained:  0.0012 - Total Variance:  0.9738\n",
      "PC62 - Variance explained:  0.0012 - Total Variance:  0.9750\n",
      "PC63 - Variance explained:  0.0012 - Total Variance:  0.9761\n",
      "PC64 - Variance explained:  0.0011 - Total Variance:  0.9773\n",
      "PC65 - Variance explained:  0.0011 - Total Variance:  0.9783\n",
      "PC66 - Variance explained:  0.0010 - Total Variance:  0.9793\n",
      "PC67 - Variance explained:  0.0010 - Total Variance:  0.9803\n",
      "PC68 - Variance explained:  0.0009 - Total Variance:  0.9813\n",
      "PC69 - Variance explained:  0.0009 - Total Variance:  0.9822\n",
      "PC70 - Variance explained:  0.0009 - Total Variance:  0.9831\n",
      "PC71 - Variance explained:  0.0009 - Total Variance:  0.9840\n",
      "PC72 - Variance explained:  0.0008 - Total Variance:  0.9848\n",
      "PC73 - Variance explained:  0.0008 - Total Variance:  0.9856\n",
      "PC74 - Variance explained:  0.0008 - Total Variance:  0.9864\n",
      "PC75 - Variance explained:  0.0008 - Total Variance:  0.9872\n",
      "PC76 - Variance explained:  0.0007 - Total Variance:  0.9879\n",
      "PC77 - Variance explained:  0.0007 - Total Variance:  0.9885\n",
      "PC78 - Variance explained:  0.0007 - Total Variance:  0.9892\n",
      "PC79 - Variance explained:  0.0006 - Total Variance:  0.9898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "W_train, v_train = np.linalg.eig(X_train_scaled.T @ X_train_scaled)\n",
    "vexp_train = W_train/W_train.sum()\n",
    "n_comps=80\n",
    "pca_train= PCA(n_components=n_comps)\n",
    "pca_train.fit(X_train_scaled)\n",
    "tve=0\n",
    "for i, ve in enumerate(pca_train.explained_variance_ratio_):\n",
    "    tve+=ve\n",
    "    print(\"PC%d - Variance explained: %7.4f - Total Variance: %7.4f\" % (i, ve, tve) )\n",
    "print()\n",
    "\n",
    "#! Doing PCA for the X_test. Not sure if it is done this way \n",
    "W_test, v_test = np.linalg.eig(X_test_scaled.T @ X_test_scaled)\n",
    "vexp_test = W_test/W_test.sum()\n",
    "pca_test = PCA(n_components=n_comps)\n",
    "pca_test.fit(X_test_scaled)\n",
    "tve2=0\n",
    "for i, ve in enumerate(pca_test.explained_variance_ratio_):\n",
    "    tve2+=ve\n",
    "    print(\"PC%d - Variance explained: %7.4f - Total Variance: %7.4f\" % (i, ve, tve2) )\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14246, 80), (7017, 80))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_PCA = X_train_scaled @ v_train[:n_comps,:].T\n",
    "X_test_PCA = X_test_scaled @ v_test[:n_comps,:].T\n",
    "(X_train_PCA.shape,X_test_PCA.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 2 - Create a Regression and classification model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Make a regression model. We are using Linear Regression and Decision Tree\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1 Using the full dataset to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse for the Liner Regression Model is 23\n",
      "The rmse for the Decion Tree Model is 12\n",
      "[ 0.48322108 -0.59234547  0.61345726 -4.77617039  0.29963382]\n",
      "[0.50159486 0.5330143  0.81785728 0.66894789 0.37498427]\n"
     ]
    }
   ],
   "source": [
    "lr_model = LinearRegression()\n",
    "dt_model = DecisionTreeRegressor()\n",
    "\n",
    "lr_model.fit(X_train_scaled,y_train)\n",
    "dt_model.fit(X_train_scaled,y_train)\n",
    "\n",
    "lr_preds = lr_model.predict(X_test_scaled)\n",
    "dt_preds = dt_model.predict(X_test_scaled)\n",
    "\n",
    "lr_rmse = mean_squared_error(y_test, lr_preds)\n",
    "dt_rmse = mean_squared_error(y_test, dt_preds)\n",
    "\n",
    "print(\"The rmse for the Liner Regression Model is %d\"%(np.sqrt(lr_rmse)))\n",
    "print(\"The rmse for the Decion Tree Model is %d\"%(np.sqrt(dt_rmse)))\n",
    "lr_scores = cross_val_score(lr_model, X, y, cv=5)\n",
    "print(lr_scores)\n",
    "dt_scores = cross_val_score(dt_model, X, y, cv=5)\n",
    "print(dt_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2 Using the projection of the full dataset to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse for the Liner Regression Model is 148\n",
      "The rmse for the Decion Tree Model is 50\n",
      "[ 0.48322108 -0.59234547  0.61345726 -4.77617039  0.29963382]\n",
      "[0.50295648 0.54937884 0.81757894 0.65682352 0.38198869]\n"
     ]
    }
   ],
   "source": [
    "lr_model2 = LinearRegression()\n",
    "dt_model2 = DecisionTreeRegressor()\n",
    "\n",
    "lr_model2.fit(X_train_PCA,y_train)\n",
    "dt_model2.fit(X_train_PCA,y_train)\n",
    "\n",
    "lr2_preds = lr_model2.predict(X_test_PCA)\n",
    "dt2_preds = dt_model2.predict(X_test_PCA)\n",
    "\n",
    "lr2_rmse = mean_squared_error(y_test, lr2_preds)\n",
    "dt2_rmse = mean_squared_error(y_test, dt2_preds)\n",
    "\n",
    "print(\"The rmse for the Liner Regression Model is %d\"%(np.sqrt(lr2_rmse)))\n",
    "print(\"The rmse for the Decion Tree Model is %d\"%(np.sqrt(dt2_rmse)))\n",
    "lr2_scores = cross_val_score(lr_model2, X, y, cv=5)\n",
    "print(lr2_scores)\n",
    "dt2_scores = cross_val_score(dt_model2, X, y, cv=5)\n",
    "print(dt2_scores)\n",
    "\n",
    "#! Problems:  \n",
    "#! Why are the models trained with the full dataset getting better results than the ones trained with the projection? \n",
    "#! Is the PCA done correctly?\n",
    "#! Why are the rsme so high"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Make a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
